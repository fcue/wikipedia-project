{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function to convert Wikipedia revision ID to Wikidata ID\n",
    "\n",
    "For example, if you pass the revision ID 935784560 (corresponding with this revision of the English Wikipedia article for N. K. Jemisin: https://en.wikipedia.org/w/index.php?oldid=935784560) to revid_to_qid, then the function should return Q2427544.\n",
    "\n",
    "* You can use the MediaWiki pageprops API to get this information: https://www.mediawiki.org/wiki/API:Pageprops\n",
    "* Note that the API accepts a revids parameter: https://www.mediawiki.org/w/api.php?action=help&modules=query\n",
    "* The sample code here might help you with getting started: https://www.mediawiki.org/wiki/API:Pageprops#Sample_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "def revid_to_qid(revid, lang):\n",
    "    \"\"\"Takes a Wikipedia article revision ID and returns the corresponding Wikidata ID\n",
    "    \n",
    "    Args:\n",
    "        revid: integer revision ID associated with an article in the provided language\n",
    "        lang: the Wikiepdia language version -- e.g., 'en' corresponds with English Wikipedia\n",
    "        \n",
    "    Returns:\n",
    "        qid: Wikidata ID associated with the article corresponding to the revision ID\n",
    "    \n",
    "    \"\"\"\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchcomplete': '', 'query': {'pages': {'29828568': {'pageid': 29828568, 'ns': 0, 'title': 'N. K. Jemisin', 'pageprops': {'defaultsort': 'Jemisin, N. K.', 'page_image_free': 'N._K._Jemisin_(cropped).jpg', 'wikibase-shortdesc': 'American science fiction and fantasy writer', 'wikibase_item': 'Q2427544'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# first try: using requests library\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"pageprops\",\n",
    "    \"revids\": \"935784560\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2427544\n"
     ]
    }
   ],
   "source": [
    "# second try: looping through a series of dictionary keys to get to the wanted id \n",
    "# seems long and redundant? \n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"pageprops\",\n",
    "    \"revids\": \"935784560\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "print(DATA[\"query\"][\"pages\"][\"29828568\"][\"pageprops\"][\"wikibase_item\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2427544\n"
     ]
    }
   ],
   "source": [
    "# set qid\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"pageprops\",\n",
    "    \"revids\": \"935784560\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "qid = DATA[\"query\"][\"pages\"][\"29828568\"][\"pageprops\"][\"wikibase_item\"]\n",
    "print(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-0dfe7c3a2cc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mqid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrevid_to_qid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrevid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m935784560\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-0dfe7c3a2cc6>\u001b[0m in \u001b[0;36mrevid_to_qid\u001b[1;34m(revid, lang)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mDATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mqid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"query\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pages\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"29828568\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pageprops\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"wikibase_item\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mqid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query'"
     ]
    }
   ],
   "source": [
    "def revid_to_qid(revid, lang):\n",
    "    lang = \"en\"\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"pageprops\",\n",
    "        \"revids\": \"revid\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "\n",
    "    qid = DATA[\"query\"][\"pages\"][\"29828568\"][\"pageprops\"][\"wikibase_item\"]\n",
    "    return qid\n",
    "    \n",
    "revid_to_qid(935784560, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function to gather Wikidata claims for a Wikidata item\n",
    "\n",
    "For example, if you pass the Wikidata ID Q2427544, then the claims and identifiers -- i.e. the properties and values listed on the page here (https://www.wikidata.org/wiki/Q2427544) -- should be returned as a sequence of (property, value) tuples as follows:\n",
    "\n",
    "[(P31, Q5), (P18, ), (P21, Q6581072), ... , (P7400, ), (P7704, )]\n",
    "\n",
    "A complete example for the item associated with the Shintokyo Maru https://www.wikidata.org/wiki/Q52329548:\n",
    "\n",
    "[(P31, Q11446), (P458, ), (P373, ), (P729, ), (P176, Q11309941), (P18, ), (P7782, Q83904766), (P2043, ), (P1093, )]\n",
    "\n",
    "Notes:\n",
    "\n",
    "Order of claims does not matter\n",
    "When there are multiple values for a property, they should all be included -- e.g., \"languages spoken, written or signed\" under https://www.wikidata.org/wiki/Q2427544 would be represented as ..., (P1412, Q7976), (P1412, Q1860), ...\n",
    "You don't need to include references\n",
    "When the value for a property is not a Wikidata item -- i.e. not entity-type of 'wikibase-entityid' -- then you can leave the value slot in the tuple blank\n",
    "\n",
    "You should try to use the mwbase Python package for gathering this information (https://pypi.org/project/mwbase/)\n",
    "\n",
    "Alternatively, you try to use the Wikidata API to get this information: https://www.mediawiki.org/wiki/Wikibase/API and more specifically https://www.wikidata.org/w/api.php?action=help&modules=wbgetclaims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qid_to_claims(qid):\n",
    "    \"\"\"Takes a Wikidata ID and returns a sequence of claims.\n",
    "    \n",
    "    Args:\n",
    "        qid: Wikidata ID\n",
    "    Returns:\n",
    "        claims: Sequence of claims tuples of form (property, value) or (property, ) when the value does not have a QID\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function to convert the claims in a Wikidata item into a document embedding\n",
    "\n",
    "For example, given a Wikidata item's sequence of claims, return an embedding that represents that item. For the purpose of this application, individual embeddings for properties and values on Wikidata will be provided and the document embedding will be defined as the average of these individual embeddings.\n",
    "\n",
    "Toy example: if a Wikidata item has the following claims [(P1, Q1), (P1, Q2), (P2, )] and these properties are associated with the following 3-dimensionnal embeddings:\n",
    "\n",
    "* P1: [1, 2, 3]\n",
    "* P2: [4, 5, 6]\n",
    "* Q1: [3, 2, 1]\n",
    "* Q2: [1, 0, 1]\n",
    "\n",
    "Then, the document embedding for this Wikidata item would be:\n",
    "\n",
    "* (P1 + Q1 + P1 + Q2 + P2) / 5, which ends up being:\n",
    "* ([1, 2, 3] + [3, 2, 1] + [1, 2, 3] + [1, 0, 1] + [4, 5, 6]) / 5, which ends up being:\n",
    "* [10, 11, 14] / 5 = [2, 2.2, 2.8]\n",
    "\n",
    "If there is no embedding for a given property (P#) or entity (Q#) -- i.e. out-of-vocabulary -- an embedding of 0s ([0.0, 0.0, 0.0] in the above example) should be used as part of the averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claims_to_doc_embedding(claims, embeddings):\n",
    "    \"\"\"Takes a sequence of Wikidata claims and produces a document embedding.\n",
    "    \n",
    "    Args:\n",
    "        claims: sequence of Wikidata claims.\n",
    "        embeddings: look-up for the embeddings associated with each property/entity in the claims.\n",
    "    Returns:\n",
    "        document embedding: sequence of floats that is the average of the claims embeddings\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "At this stage, for topic classification, we would train a machine learning model that predicts a Wikidata item's topics based on the document embedding. We will leave that step off for now and instead focus on evaluating the document embeddings that the preceding code generates:\n",
    "\n",
    "Choose some similar entities that have Wikipedia articles and compare how similar their document embeddings are. Also compare them to entities that are sorta similar and entities that are very different.\n",
    "\n",
    "Do the embeddings capture your beliefs about how similar given entities are?\n",
    "\n",
    "Can you find examples where the similarity of document embeddings does not match what you would expect? Why?\n",
    "For example, you might imagine that the document embedding for Ernest Shackleton (Wikipedia; Wikidata) is similar to Antarctica (Wikipedia; Wikidata) or even Greenland because it is quite arctic (Wikipedia; Wikidata) but quite different than Tanzania (Wikipedia; Wikidata). However, Antarctica and Tanzania should still be relatively similar to each other given that they are both geographic regions.\n",
    "\n",
    "For similarity, you can use the cosine similarity of two document embeddings. With cosine similarity, values close to 1 indicate that the two documents are very similar and values close to 0 indicate that the documents have little in common. Feel free to compute it yourself or find a Python package that can help.\n",
    "\n",
    "For embeddings, you may use the embeddings found within the model.bin file here: https://github.com/geohci/wikidata-topic-model-api/blob/master/models/model.bin or https://drive.google.com/file/d/1YAniioZAtMHMMRWbA7HrbuSZVUi5QEpQ/view?usp=sharing (both files are the same, just hosted on different platforms). This is a trained fastText model that contains embeddings for Wikidata claims. You can download that file and upload it to your PAWS directory so that it is available from this notebook. You can then use the functions that you wrote earlier to convert the Wikidata items you are comparing to document embeddings that can be evaluated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "\n",
    "import fasttext\n",
    "\n",
    "# See: https://fasttext.cc/docs/en/python-module.html#model-object\n",
    "model = fasttext.load_model('model.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
